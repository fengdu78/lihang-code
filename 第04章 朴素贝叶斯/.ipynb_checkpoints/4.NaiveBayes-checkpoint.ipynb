{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4章 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1．朴素贝叶斯法是典型的生成学习方法。生成方法由训练数据学习联合概率分布\n",
    "$P(X,Y)$，然后求得后验概率分布$P(Y|X)$。具体来说，利用训练数据学习$P(X|Y)$和$P(Y)$的估计，得到联合概率分布：\n",
    "\n",
    "$$P(X,Y)＝P(Y)P(X|Y)$$\n",
    "\n",
    "概率估计方法可以是极大似然估计或贝叶斯估计。\n",
    "\n",
    "2．朴素贝叶斯法的基本假设是条件独立性，\n",
    "\n",
    "$$\\begin{aligned} P(X&=x | Y=c_{k} )=P\\left(X^{(1)}=x^{(1)}, \\cdots, X^{(n)}=x^{(n)} | Y=c_{k}\\right) \\\\ &=\\prod_{j=1}^{n} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right) \\end{aligned}$$\n",
    "\n",
    "\n",
    "这是一个较强的假设。由于这一假设，模型包含的条件概率的数量大为减少，朴素贝叶斯法的学习与预测大为简化。因而朴素贝叶斯法高效，且易于实现。其缺点是分类的性能不一定很高。\n",
    "\n",
    "3．朴素贝叶斯法利用贝叶斯定理与学到的联合概率模型进行分类预测。\n",
    "\n",
    "$$P(Y | X)=\\frac{P(X, Y)}{P(X)}=\\frac{P(Y) P(X | Y)}{\\sum_{Y} P(Y) P(X | Y)}$$\n",
    " \n",
    "将输入$x$分到后验概率最大的类$y$。\n",
    "\n",
    "$$y=\\arg \\max _{c_{k}} P\\left(Y=c_{k}\\right) \\prod_{j=1}^{n} P\\left(X_{j}=x^{(j)} | Y=c_{k}\\right)$$\n",
    "\n",
    "后验概率最大等价于0-1损失函数时的期望风险最小化。\n",
    "\n",
    "\n",
    "模型：\n",
    "\n",
    "- 高斯模型\n",
    "- 多项式模型\n",
    "- 伯努利模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "def create_data():\n",
    "    iris = load_iris()\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    df['label'] = iris.target\n",
    "    df.columns = [\n",
    "        'sepal length', 'sepal width', 'petal length', 'petal width', 'label'\n",
    "    ]\n",
    "    data = np.array(df.iloc[:100, :])\n",
    "    print(data)\n",
    "    return data[:, :-1], data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2 0. ]\n",
      " [4.9 3.  1.4 0.2 0. ]\n",
      " [4.7 3.2 1.3 0.2 0. ]\n",
      " [4.6 3.1 1.5 0.2 0. ]\n",
      " [5.  3.6 1.4 0.2 0. ]\n",
      " [5.4 3.9 1.7 0.4 0. ]\n",
      " [4.6 3.4 1.4 0.3 0. ]\n",
      " [5.  3.4 1.5 0.2 0. ]\n",
      " [4.4 2.9 1.4 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [5.4 3.7 1.5 0.2 0. ]\n",
      " [4.8 3.4 1.6 0.2 0. ]\n",
      " [4.8 3.  1.4 0.1 0. ]\n",
      " [4.3 3.  1.1 0.1 0. ]\n",
      " [5.8 4.  1.2 0.2 0. ]\n",
      " [5.7 4.4 1.5 0.4 0. ]\n",
      " [5.4 3.9 1.3 0.4 0. ]\n",
      " [5.1 3.5 1.4 0.3 0. ]\n",
      " [5.7 3.8 1.7 0.3 0. ]\n",
      " [5.1 3.8 1.5 0.3 0. ]\n",
      " [5.4 3.4 1.7 0.2 0. ]\n",
      " [5.1 3.7 1.5 0.4 0. ]\n",
      " [4.6 3.6 1.  0.2 0. ]\n",
      " [5.1 3.3 1.7 0.5 0. ]\n",
      " [4.8 3.4 1.9 0.2 0. ]\n",
      " [5.  3.  1.6 0.2 0. ]\n",
      " [5.  3.4 1.6 0.4 0. ]\n",
      " [5.2 3.5 1.5 0.2 0. ]\n",
      " [5.2 3.4 1.4 0.2 0. ]\n",
      " [4.7 3.2 1.6 0.2 0. ]\n",
      " [4.8 3.1 1.6 0.2 0. ]\n",
      " [5.4 3.4 1.5 0.4 0. ]\n",
      " [5.2 4.1 1.5 0.1 0. ]\n",
      " [5.5 4.2 1.4 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.2 0. ]\n",
      " [5.  3.2 1.2 0.2 0. ]\n",
      " [5.5 3.5 1.3 0.2 0. ]\n",
      " [4.9 3.6 1.4 0.1 0. ]\n",
      " [4.4 3.  1.3 0.2 0. ]\n",
      " [5.1 3.4 1.5 0.2 0. ]\n",
      " [5.  3.5 1.3 0.3 0. ]\n",
      " [4.5 2.3 1.3 0.3 0. ]\n",
      " [4.4 3.2 1.3 0.2 0. ]\n",
      " [5.  3.5 1.6 0.6 0. ]\n",
      " [5.1 3.8 1.9 0.4 0. ]\n",
      " [4.8 3.  1.4 0.3 0. ]\n",
      " [5.1 3.8 1.6 0.2 0. ]\n",
      " [4.6 3.2 1.4 0.2 0. ]\n",
      " [5.3 3.7 1.5 0.2 0. ]\n",
      " [5.  3.3 1.4 0.2 0. ]\n",
      " [7.  3.2 4.7 1.4 1. ]\n",
      " [6.4 3.2 4.5 1.5 1. ]\n",
      " [6.9 3.1 4.9 1.5 1. ]\n",
      " [5.5 2.3 4.  1.3 1. ]\n",
      " [6.5 2.8 4.6 1.5 1. ]\n",
      " [5.7 2.8 4.5 1.3 1. ]\n",
      " [6.3 3.3 4.7 1.6 1. ]\n",
      " [4.9 2.4 3.3 1.  1. ]\n",
      " [6.6 2.9 4.6 1.3 1. ]\n",
      " [5.2 2.7 3.9 1.4 1. ]\n",
      " [5.  2.  3.5 1.  1. ]\n",
      " [5.9 3.  4.2 1.5 1. ]\n",
      " [6.  2.2 4.  1.  1. ]\n",
      " [6.1 2.9 4.7 1.4 1. ]\n",
      " [5.6 2.9 3.6 1.3 1. ]\n",
      " [6.7 3.1 4.4 1.4 1. ]\n",
      " [5.6 3.  4.5 1.5 1. ]\n",
      " [5.8 2.7 4.1 1.  1. ]\n",
      " [6.2 2.2 4.5 1.5 1. ]\n",
      " [5.6 2.5 3.9 1.1 1. ]\n",
      " [5.9 3.2 4.8 1.8 1. ]\n",
      " [6.1 2.8 4.  1.3 1. ]\n",
      " [6.3 2.5 4.9 1.5 1. ]\n",
      " [6.1 2.8 4.7 1.2 1. ]\n",
      " [6.4 2.9 4.3 1.3 1. ]\n",
      " [6.6 3.  4.4 1.4 1. ]\n",
      " [6.8 2.8 4.8 1.4 1. ]\n",
      " [6.7 3.  5.  1.7 1. ]\n",
      " [6.  2.9 4.5 1.5 1. ]\n",
      " [5.7 2.6 3.5 1.  1. ]\n",
      " [5.5 2.4 3.8 1.1 1. ]\n",
      " [5.5 2.4 3.7 1.  1. ]\n",
      " [5.8 2.7 3.9 1.2 1. ]\n",
      " [6.  2.7 5.1 1.6 1. ]\n",
      " [5.4 3.  4.5 1.5 1. ]\n",
      " [6.  3.4 4.5 1.6 1. ]\n",
      " [6.7 3.1 4.7 1.5 1. ]\n",
      " [6.3 2.3 4.4 1.3 1. ]\n",
      " [5.6 3.  4.1 1.3 1. ]\n",
      " [5.5 2.5 4.  1.3 1. ]\n",
      " [5.5 2.6 4.4 1.2 1. ]\n",
      " [6.1 3.  4.6 1.4 1. ]\n",
      " [5.8 2.6 4.  1.2 1. ]\n",
      " [5.  2.3 3.3 1.  1. ]\n",
      " [5.6 2.7 4.2 1.3 1. ]\n",
      " [5.7 3.  4.2 1.2 1. ]\n",
      " [5.7 2.9 4.2 1.3 1. ]\n",
      " [6.2 2.9 4.3 1.3 1. ]\n",
      " [5.1 2.5 3.  1.1 1. ]\n",
      " [5.7 2.8 4.1 1.3 1. ]]\n"
     ]
    }
   ],
   "source": [
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5. , 3.4, 1.5, 0.2]), 0.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0], y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/\n",
    "http://www.zhangzhenhu.com/probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id5\n",
    "\n",
    "## GaussianNB 高斯朴素贝叶斯\n",
    "朴素贝叶斯模型中，假设所有所有特征变量都是离散变量，然而实际应用场景是复杂多变的，很多时候是无法满足这样的强假设的。如果特征变量$X$是连续值怎么办？我们可以通过离散化的手段把连续值转换成离散变量后，再应用朴素贝叶斯模型进行处理。但这样做显然不够优雅，事实上，贝叶斯分类器并没有约束变量服从何种概率分布，理论上任何概率分布都是支持的，无论是标签变量Y ，还是特征变量$X$都可以是任意类型的分布。假设特征变量$X$是高斯分布，标签变量$Y$仍然是离散（类别）变量的分类模型， 这类模型有个名字，称之为高斯判别分析（Gaussian Discriminant Analysis,GDA)，或者高斯朴素贝叶斯（GaussianNB）\n",
    "\n",
    "特征变量$x$中每个特征都为高斯概率密度函数（完全条件独立）：\n",
    "$$P(x_i | y_k)=\\frac{1}{\\sqrt{2\\pi\\sigma^2_{yk}}}exp(-\\frac{(x_i-\\mu_{yk})^2}{2\\sigma^2_{yk}})$$\n",
    "\n",
    "数学期望(mean)：$\\mu$\n",
    "\n",
    "方差：$\\sigma^2=\\frac{\\sum(X-\\mu)^2}{N}$\n",
    "\n",
    "类似于朴素贝叶斯模型的参数估计过程，可以把观测样本集按照Y的值划分成两份，用每一份数据分别去估计上述两个高斯分布的参数即可。\n",
    ">将输入$x$分到后验概率最大的类$y$。\n",
    "$$y=\\arg \\max _{yk} P\\left(Y=yk\\right) \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2_{yk}}}exp(-\\frac{(x_i-\\mu_{yk})^2}{2\\sigma^2_{yk}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    # 数学期望\n",
    "    @staticmethod\n",
    "    def mean(X):\n",
    "        return sum(X) / float(len(X))\n",
    "\n",
    "    # 标准差（方差）\n",
    "    def stdev(self, X):\n",
    "        avg = self.mean(X)\n",
    "        return math.sqrt(sum([pow(x - avg, 2) for x in X]) / float(len(X)))\n",
    "\n",
    "    # 高斯概率密度函数\n",
    "    def gaussian_probability(self, x, mean, stdev):\n",
    "        exponent = math.exp(-(math.pow(x - mean, 2) /\n",
    "                              (2 * math.pow(stdev, 2))))\n",
    "        return (1 / (math.sqrt(2 * math.pi) * stdev)) * exponent\n",
    "\n",
    "    # 处理X_train\n",
    "    def summarize(self, train_data):\n",
    "        summaries = [(self.mean(i), self.stdev(i)) for i in zip(*train_data)]\n",
    "        return summaries\n",
    "\n",
    "    # 分类别求出数学期望和标准差\n",
    "    def fit(self, X, y):\n",
    "        # 所有labels类型\n",
    "        labels = list(set(y))\n",
    "        # 创建label与[特征期望，特征标准差]的字典映射\n",
    "        data = {label: [] for label in labels}\n",
    "        for f, label in zip(X, y):\n",
    "            data[label].append(f)\n",
    "        # value为X的多(n)维特征,生成n个[特征期望，特征标准差]\n",
    "        self.model = {\n",
    "            label: self.summarize(value)\n",
    "            for label, value in data.items()\n",
    "        }\n",
    "        print(self.model)\n",
    "        return 'gaussianNB train done!'\n",
    "\n",
    "    # 计算概率\n",
    "    def calculate_probabilities(self, input_data):\n",
    "        # summaries:{0.0: [(5.0, 0.37),(3.42, 0.40)], 1.0: [(5.8, 0.449),(2.7, 0.27)]}\n",
    "        # input_data:[1.1, 2.2]\n",
    "        probabilities = {}\n",
    "        for label, value in self.model.items():\n",
    "            probabilities[label] = 1\n",
    "            for i in range(len(value)):\n",
    "                mean, stdev = value[i]\n",
    "                probabilities[label] *= self.gaussian_probability(\n",
    "                    input_data[i], mean, stdev)\n",
    "        return probabilities\n",
    "\n",
    "    # 类别\n",
    "    def predict(self, X_test):\n",
    "        # {0.0: 2.9680340789325763e-27, 1.0: 3.5749783019849535e-26}\n",
    "        label = sorted(\n",
    "            self.calculate_probabilities(X_test).items(),\n",
    "            key=lambda x: x[-1])[-1][0]\n",
    "        return label\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        right = 0\n",
    "        for X, y in zip(X_test, y_test):\n",
    "            label = self.predict(X)\n",
    "            if label == y:\n",
    "                right += 1\n",
    "\n",
    "        return right / float(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: [(5.042424242424242, 0.3709368181357421), (3.4818181818181815, 0.3545843024560018), (1.4484848484848483, 0.17077579155885297), (0.2424242424242425, 0.08886592908251627)], 1.0: [(5.932432432432432, 0.4644404096149176), (2.7540540540540546, 0.31928517677021306), (4.232432432432433, 0.4702236696389048), (1.3135135135135134, 0.19194898275205088)]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gaussianNB train done!'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([4.4,  3.2,  1.3,  0.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### scikit-learn实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[4.4,  3.2,  1.3,  0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB # 伯努利模型和多项式模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第4章朴素贝叶斯法-习题\n",
    "\n",
    "### 习题4.1\n",
    "&emsp;&emsp;用极大似然估计法推出朴素贝叶斯法中的概率估计公式(4.8)及公式 (4.9)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**  \n",
    "**第1步：**证明公式(4.8)：$\\displaystyle P(Y=c_k) = \\frac{\\displaystyle \\sum_{i=1}^N I(y_i=c_k)}{N}$  \n",
    "由于朴素贝叶斯法假设$Y$是定义在输出空间$\\mathcal{Y}$上的随机变量，因此可以定义$P(Y=c_k)$概率为$p$。  \n",
    "令$\\displaystyle m=\\sum_{i=1}^NI(y_i=c_k)$，得出似然函数：$$L(p)=f_D(y_1,y_2,\\cdots,y_n|\\theta)=\\binom{N}{m}p^m(1-p)^{(N-m)}$$使用微分求极值，两边同时对$p$求微分：$$\\begin{aligned}\n",
    "0 &= \\binom{N}{m}\\left[mp^{(m-1)}(1-p)^{(N-m)}-(N-m)p^m(1-p)^{(N-m-1)}\\right] \\\\\n",
    "& = \\binom{N}{m}\\left[p^{(m-1)}(1-p)^{(N-m-1)}(m-Np)\\right]\n",
    "\\end{aligned}$$可求解得到$\\displaystyle p=0,p=1,p=\\frac{m}{N}$  \n",
    "显然$\\displaystyle P(Y=c_k)=p=\\frac{m}{N}=\\frac{\\displaystyle \\sum_{i=1}^N I(y_i=c_k)}{N}$，公式(4.8)得证。\n",
    "\n",
    "----\n",
    "\n",
    "**第2步：**证明公式(4.9)：$\\displaystyle P(X^{(j)}=a_{jl}|Y=c_k) = \\frac{\\displaystyle \\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y_i=c_k)}{\\displaystyle \\sum_{i=1}^N I(y_i=c_k)}$  \n",
    "令$P(X^{(j)}=a_{jl}|Y=c_k)=p$，令$\\displaystyle m=\\sum_{i=1}^N I(y_i=c_k), q=\\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y_i=c_k)$，得出似然函数：$$L(p)=\\binom{m}{q}p^q(i-p)^{m-q}$$使用微分求极值，两边同时对$p$求微分：$$\\begin{aligned}\n",
    "0 &= \\binom{m}{q}\\left[qp^{(q-1)}(1-p)^{(m-q)}-(m-q)p^q(1-p)^{(m-q-1)}\\right] \\\\\n",
    "& = \\binom{m}{q}\\left[p^{(q-1)}(1-p)^{(m-q-1)}(q-mp)\\right]\n",
    "\\end{aligned}$$可求解得到$\\displaystyle p=0,p=1,p=\\frac{q}{m}$  \n",
    "显然$\\displaystyle P(X^{(j)}=a_{jl}|Y=c_k)=p=\\frac{q}{m}=\\frac{\\displaystyle \\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y_i=c_k)}{\\displaystyle \\sum_{i=1}^N I(y_i=c_k)}$，公式(4.9)得证。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 习题4.2\n",
    "&emsp;&emsp;用贝叶斯估计法推出朴素贝叶斯法中的概率估计公式(4.10)及公式(4.11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**  \n",
    "**第1步：**证明公式(4.11)：$\\displaystyle P(Y=c_k) = \\frac{\\displaystyle \\sum_{i=1}^N I(y_i=c_k) + \\lambda}{N+K \\lambda}$  \n",
    "加入先验概率，在没有任何信息的情况下，可以假设先验概率为均匀概率（即每个事件的概率是相同的）。  \n",
    "可得$\\displaystyle p=\\frac{1}{K} \\Leftrightarrow pK-1=0\\quad(1)$  \n",
    "根据习题4.1得出先验概率的极大似然估计是$\\displaystyle pN - \\sum_{i=1}^N I(y_i=c_k) = 0\\quad(2)$  \n",
    "存在参数$\\lambda$使得$(1) \\cdot \\lambda + (2) = 0$  \n",
    "所以有$$\\lambda(pK-1) + pN - \\sum_{i=1}^N I(y_i=c_k) = 0$$可得$\\displaystyle P(Y=c_k) = \\frac{\\displaystyle \\sum_{i=1}^N I(y_i=c_k) + \\lambda}{N+K \\lambda}$，公式(4.11)得证。  \n",
    "\n",
    "----\n",
    "\n",
    "**第2步：**证明公式(4.10)：$\\displaystyle P_{\\lambda}(X^{(j)}=a_{jl} | Y = c_k) = \\frac{\\displaystyle \\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y_i=c_k) + \\lambda}{\\displaystyle \\sum_{i=1}^N I(y_i=c_k) + S_j \\lambda}$   \n",
    "根据第1步，可同理得到$$\n",
    "P(Y=c_k, x^{(j)}=a_{j l})=\\frac{\\displaystyle \\sum_{i=1}^N I(y_i=c_k, x_i^{(j)}=a_{jl})+\\lambda}{N+K S_j \\lambda}$$  \n",
    "$$\\begin{aligned} \n",
    "P(x^{(j)}=a_{jl} | Y=c_k)\n",
    "&= \\frac{P(Y=c_k, x^{(j)}=a_{j l})}{P(y_i=c_k)} \\\\\n",
    "&= \\frac{\\displaystyle \\frac{\\displaystyle \\sum_{i=1}^N I(y_i=c_k, x_i^{(j)}=a_{jl})+\\lambda}{N+K S_j \\lambda}}{\\displaystyle \\frac{\\displaystyle \\sum_{i=1}^N I(y_i=c_k) + \\lambda}{N+K \\lambda}} \\\\\n",
    "&= (\\lambda可以任意取值，于是取\\lambda = S_j \\lambda) \\\\\n",
    "&= \\frac{\\displaystyle \\frac{\\displaystyle \\sum_{i=1}^N I(y_i=c_k, x_i^{(j)}=a_{jl})+\\lambda}{N+K S_j \\lambda}}{\\displaystyle \\frac{\\displaystyle \\sum_{i=1}^N I(y_i=c_k) + \\lambda}{N+K S_j \\lambda}} \\\\ \n",
    "&= \\frac{\\displaystyle \\sum_{i=1}^N I(y_i=c_k, x_i^{(j)}=a_{jl})+\\lambda}{\\displaystyle \\sum_{i=1}^N I(y_i=c_k) + \\lambda} (其中\\lambda = S_j \\lambda)\\\\\n",
    "&= \\frac{\\displaystyle \\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y_i=c_k) + \\lambda}{\\displaystyle \\sum_{i=1}^N I(y_i=c_k) + S_j \\lambda}\n",
    "\\end{aligned} $$  \n",
    "公式(4.11)得证。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "参考代码：https://github.com/wzyonggege/statistical-learning-method\n",
    "\n",
    "本文代码更新地址：https://github.com/fengdu78/lihang-code\n",
    "\n",
    "习题解答：https://github.com/datawhalechina/statistical-learning-method-solutions-manual\n",
    "\n",
    "中文注释制作：机器学习初学者公众号：ID:ai-start-com\n",
    "\n",
    "配置环境：python 3.5+\n",
    "\n",
    "代码全部测试通过。\n",
    "![gongzhong](../gongzhong.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
